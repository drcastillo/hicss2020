{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import sys\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "(55000, 28, 14, 1)\n"
     ]
    }
   ],
   "source": [
    "# MNIST DATA\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "X = np.expand_dims(mnist.train._images.reshape(55000,28, 28), 3)\n",
    "Y = mnist.train._labels\n",
    "\n",
    "X1 = X[:,:,:14]\n",
    "X2 = X[:,:,14:]\n",
    "\n",
    "print X1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double input stream MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hiranumn/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:21: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Merge, Conv2D, Dropout, MaxPooling2D, Flatten\n",
    "\n",
    "model1_left = Sequential()\n",
    "model1_left.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(28, 14, 1))) # (28, 28, 1) for tf and (1, 28, 28) for theano\n",
    "model1_left.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model1_left.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1_left.add(Dropout(0.25))\n",
    "\n",
    "model1_right = Sequential()\n",
    "model1_right.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(28, 14, 1))) # (28, 28, 1) for tf and (1, 28, 28) for theano\n",
    "model1_right.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model1_right.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1_right.add(Dropout(0.25))\n",
    "\n",
    "merged = Merge([model1_left, model1_right], mode='concat')\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(merged)\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(128, activation='relu'))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(10, activation='softmax'))\n",
    "model1.compile(optimizer='sgd', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1393b99450>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit([X1, X2], Y, epochs=15, batch_size=128, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted1 = model1.predict([X1, X2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABaCAYAAACouzjNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncwge0ByCBzSH4AHNIXhA/wMxoL/NTuE6bQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1393b1d210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 0\n",
    "\n",
    "print \"prediction:\", np.argmax(predicted1[index])\n",
    "plt.figure(figsize=(1,2))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(X1[index, :, :, 0])\n",
    "plt.xticks([],[])\n",
    "plt.yticks([],[])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(X2[index, :, :, 0])\n",
    "plt.xticks([],[])\n",
    "plt.yticks([],[])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Merge, Input\n",
    "\n",
    "l_in = Input(shape=(28, 14, 1), )\n",
    "l = Conv2D(32, kernel_size=(3, 3), activation='relu')(l_in)\n",
    "l = Conv2D(64, (3, 3), activation='relu')(l)\n",
    "l = MaxPooling2D(pool_size=(2, 2))(l)\n",
    "l = Dropout(0.25)(l)\n",
    "\n",
    "l2_in = Input(shape=(28, 14, 1), )\n",
    "l2 = Conv2D(32, kernel_size=(3, 3), activation='relu') (l2_in)\n",
    "l2 = Conv2D(64, (3, 3), activation='relu')(l2)\n",
    "l2 = MaxPooling2D(pool_size=(2, 2))(l2)\n",
    "l2 = Dropout(0.25)(l2)\n",
    "\n",
    "x = keras.layers.concatenate([l, l2])\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x_out = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model2 = Model(inputs=[l_in, l2_in], outputs=x_out)\n",
    "model2.compile(optimizer='sgd', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2.fit([X1, X2], Y, epochs=15, batch_size=128, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted2 = model2.predict([X1, X2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "\n",
    "print \"prediction:\", np.argmax(predicted2[index])\n",
    "plt.figure(figsize=(1,2))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(X1[index, :, :, 0])\n",
    "plt.xticks([],[])\n",
    "plt.yticks([],[])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(X2[index, :, :, 0])\n",
    "plt.xticks([],[])\n",
    "plt.yticks([],[])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrated gradients code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "# Implemented by Naozumi Hiranuma (hiranumn@uw.edu)            #\n",
    "#                                                              #\n",
    "# Kears-compatible implmentation of Integrated Gradients       # \n",
    "# proposed in \"Axiomatic attribution for deep neuron networks\" #\n",
    "# (https://arxiv.org/abs/1703.01365).                          #\n",
    "#                                                              #\n",
    "# Keywords: Shapley values, interpretable machine learning     #\n",
    "################################################################\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import sys\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "'''\n",
    "Integrated gradients approximates Shapley values by integrating partial\n",
    "gradients with respect to input features from reference input to the\n",
    "actual input. The following class implements this concept.\n",
    "'''\n",
    "class integrated_gradients:\n",
    "    # model: Keras model that you wish to explain.\n",
    "    # outchannels: In case the model are multi tasking, you can specify which channels you want.\n",
    "    def __init__(self, model, outchannels=[], verbose=1):\n",
    "    \n",
    "        # Bacnend: either tensorflow or theano)\n",
    "        self.backend = K.backend()\n",
    "        \n",
    "        #load model supports keras.Model and keras.Sequential\n",
    "        if isinstance(model, Sequential):\n",
    "            self.model = model.model\n",
    "        elif isinstance(model, Model):\n",
    "            self.model = model\n",
    "        else:\n",
    "            print \"Invalid input model\"\n",
    "            return -1\n",
    "        \n",
    "        #load input tensors\n",
    "        self.input_tensors = []\n",
    "        for i in self.model.inputs:\n",
    "            self.input_tensors.append(i)\n",
    "        # The learning phase flag is a bool tensor (0 = test, 1 = train)\n",
    "        # to be passed as input to any Keras function that uses \n",
    "        # a different behavior at train time and test time.\n",
    "        self.input_tensors.append(K.learning_phase())\n",
    "        \n",
    "        #If outputchanel is specified, use it.\n",
    "        #Otherwise evalueate all outputs.\n",
    "        self.outchannels = outchannels\n",
    "        if len(self.outchannels) == 0: \n",
    "            if verbose: print \"Evaluated output channel (0-based index): All\"\n",
    "            if K.backend() == \"tensorflow\":\n",
    "                self.outchannels = range(self.model.output.shape[1]._value)\n",
    "            elif K.backend() == \"theano\":\n",
    "                self.outchannels = range(model1.output._keras_shape[1])\n",
    "        else:\n",
    "            if verbose: \n",
    "                print \"Evaluated output channels (0-based index):\", \n",
    "                for i in self.outchannels: print i\n",
    "                print\n",
    "                \n",
    "        #Build gradient functions for desired output channels.\n",
    "        self.get_gradients = {}\n",
    "        if verbose: print \"Building gradient functions\"\n",
    "        \n",
    "            # Evaluate over all channels.\n",
    "        for c in self.outchannels:\n",
    "            # Get tensor that calcuates gradient\n",
    "            if K.backend() == \"tensorflow\":\n",
    "                gradients = self.model.optimizer.get_gradients(self.model.output[:, c], self.model.input)\n",
    "            if K.backend() == \"theano\":\n",
    "                gradients = self.model.optimizer.get_gradients(self.model.output[:, c].sum(), self.model.input)\n",
    "                \n",
    "            # Build computational graph that calculates the tensfor given inputs\n",
    "            self.get_gradients[c] = K.function(inputs=self.input_tensors, outputs=gradients)\n",
    "            \n",
    "            # This takes a lot of time for a big model with many tasks.\n",
    "            # So lets pring the progress.\n",
    "            if verbose:\n",
    "                sys.stdout.write('\\r')\n",
    "                sys.stdout.write(\"Progress: \"+str(int((c+1)*1.0/len(self.outchannels)*1000)*1.0/10)+\"%\")\n",
    "                sys.stdout.flush()\n",
    "        # Done\n",
    "        if verbose: print \"\\nDone.\"\n",
    "            \n",
    "                \n",
    "    '''\n",
    "    Input: sample to explain, channel to explain\n",
    "    Optional inputs:\n",
    "        - reference: reference values (defaulted to 0s).\n",
    "        - steps: # steps from reference values to the actual sample.\n",
    "    Output: list of numpy arrays to integrated over.\n",
    "    '''\n",
    "    def explain(self, sample, outc=0, reference=False, num_steps=50, verbose=0):\n",
    "        \n",
    "        # Each element for each input stream.\n",
    "        samples = []\n",
    "        numsteps = []\n",
    "        step_sizes = []\n",
    "        \n",
    "        # If multiple inputs are present, feed them as list of np arrays. \n",
    "        if isinstance(sample, list):\n",
    "            #If reference is present, reference and sample size need to be equal.\n",
    "            if reference != False: \n",
    "                assert len(sample) == len(reference)\n",
    "            for i in range(len(sample)):\n",
    "                if reference == False:\n",
    "                    _output = integrated_gradients.linearly_interpolate(sample[i], False, num_steps)\n",
    "                else:\n",
    "                    _output = integrated_gradients.linearly_interpolate(sample[i], False, num_steps)\n",
    "                samples.append(_output[0])\n",
    "                numsteps.append(_output[1])\n",
    "                step_sizes.append(_output[2])\n",
    "        \n",
    "        # Or you can feed just a single numpy arrray. \n",
    "        elif isinstance(sample, np.ndarray):\n",
    "            _output = integrated_gradients.linearly_interpolate(sample, reference, num_steps)\n",
    "            samples.append(_output[0])\n",
    "            numsteps.append(_output[1])\n",
    "            step_sizes.append(_output[2])\n",
    "            \n",
    "        # Desired channel must be in the list of outputchannels\n",
    "        assert outc in self.outchannels\n",
    "        if verbose: print \"Explaning the \"+str(self.outchannels[outc])+\"th output.\"\n",
    "            \n",
    "        # For tensorflow backend\n",
    "        _input = []\n",
    "        for s in samples:\n",
    "            _input.append(s)\n",
    "        _input.append(0)\n",
    "        \n",
    "        if K.backend() == \"tensorflow\": \n",
    "            gradients = self.get_gradients[outc](_input)\n",
    "        elif K.backend() == \"theano\":\n",
    "            gradients = self.get_gradients[outc](_input)\n",
    "            if len(self.model.inputs) == 1:\n",
    "                gradients = [gradients]\n",
    "        \n",
    "        explanation = []\n",
    "        for i in range(len(gradients)):\n",
    "            _temp = np.sum(gradients[i], axis=0)\n",
    "            explanation.append(np.multiply(_temp, step_sizes[i]))\n",
    "            \n",
    "\n",
    "        if isinstance(sample, list):\n",
    "            return explanation\n",
    "        elif isinstance(sample, np.ndarray):\n",
    "            return explanation[0]\n",
    "        return -1\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Input: numpy array of a sample\n",
    "    Optional inputs:\n",
    "        - reference: reference values (defaulted to 0s).\n",
    "        - steps: # steps from reference values to the actual sample.\n",
    "    Output: list of numpy arrays to integrated over.\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def linearly_interpolate(sample, reference=False, num_steps=50):\n",
    "        # Use default reference values if reference is not specified\n",
    "        if reference is False: reference = np.zeros(sample.shape);\n",
    "\n",
    "        # Reference and sample shape needs to match exactly\n",
    "        assert sample.shape == reference.shape\n",
    "\n",
    "        # Calcuated stepwise difference from reference to the actual sample.\n",
    "        ret = np.zeros(tuple([num_steps] +[i for i in sample.shape]))\n",
    "        for s in range(num_steps):\n",
    "            ret[s] = reference+(sample-reference)*(s*1.0/num_steps)\n",
    "\n",
    "        return ret, num_steps, (sample-reference)*(1.0/num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ig1 = integrated_gradients(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = np.random.randint(55000)\n",
    "pred = np.argmax(predicted1[index])\n",
    "print \"prediction:\", pred\n",
    "\n",
    "ex = ig1.explain([X1[index, :, :, :], X2[index, :, :, :]], outc=pred)\n",
    "th = max(np.abs(np.min([np.min(ex[0]), np.min(ex[1])])), np.abs(np.max([np.max(ex[0]), np.max(ex[1])])))\n",
    "\n",
    "plt.figure(figsize=(1,2))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(ex[0][:,:,0], cmap=\"seismic\", vmin=-1*th, vmax=th)\n",
    "plt.xticks([],[])\n",
    "plt.yticks([],[])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(ex[1][:,:,0], cmap=\"seismic\", vmin=-1*th, vmax=th)\n",
    "plt.xticks([],[])\n",
    "plt.yticks([],[])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ig2 = integrated_gradients(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = np.random.randint(55000)\n",
    "pred = np.argmax(predicted2[index])\n",
    "print \"prediction:\", pred\n",
    "\n",
    "ex = ig2.explain([X1[index, :, :, :], X2[index, :, :, :]], outc=pred)\n",
    "th = max(np.abs(np.min([np.min(ex[0]), np.min(ex[1])])), np.abs(np.max([np.max(ex[0]), np.max(ex[1])])))\n",
    "\n",
    "plt.figure(figsize=(1,2))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(ex[0][:,:,0], cmap=\"seismic\", vmin=-1*th, vmax=th)\n",
    "plt.xticks([],[])\n",
    "plt.yticks([],[])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(ex[1][:,:,0], cmap=\"seismic\", vmin=-1*th, vmax=th)\n",
    "plt.xticks([],[])\n",
    "plt.yticks([],[])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
